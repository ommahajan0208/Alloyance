{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b637f0b7-5349-49a5-ab08-424fe1db911a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7799f3c-120e-4382-8783-fc90c3c32c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-generativeai\n",
      "  Downloading google_generativeai-0.8.5-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting google-ai-generativelanguage==0.6.15 (from google-generativeai)\n",
      "  Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting google-api-core (from google-generativeai)\n",
      "  Downloading google_api_core-2.25.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting google-api-python-client (from google-generativeai)\n",
      "  Downloading google_api_python_client-2.183.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: google-auth>=2.15.0 in c:\\users\\ommah\\anaconda3\\lib\\site-packages (from google-generativeai) (2.40.3)\n",
      "Requirement already satisfied: protobuf in c:\\users\\ommah\\anaconda3\\lib\\site-packages (from google-generativeai) (5.29.3)\n",
      "Requirement already satisfied: pydantic in c:\\users\\ommah\\anaconda3\\lib\\site-packages (from google-generativeai) (2.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ommah\\anaconda3\\lib\\site-packages (from google-generativeai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\ommah\\anaconda3\\lib\\site-packages (from google-generativeai) (4.12.2)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Downloading proto_plus-1.26.1-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in c:\\users\\ommah\\anaconda3\\lib\\site-packages (from google-api-core->google-generativeai) (1.70.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.18.0 in c:\\users\\ommah\\anaconda3\\lib\\site-packages (from google-api-core->google-generativeai) (2.32.5)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in c:\\users\\ommah\\anaconda3\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai) (1.74.0)\n",
      "Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.15->google-generativeai)\n",
      "  Downloading grpcio_status-1.75.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\ommah\\anaconda3\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (5.5.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\ommah\\anaconda3\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\ommah\\anaconda3\\lib\\site-packages (from google-auth>=2.15.0->google-generativeai) (4.9.1)\n",
      "INFO: pip is looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading grpcio_status-1.74.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.73.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.73.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.72.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.72.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "  Downloading grpcio_status-1.71.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\ommah\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ommah\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ommah\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ommah\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.18.0->google-api-core->google-generativeai) (2025.4.26)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\ommah\\anaconda3\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth>=2.15.0->google-generativeai) (0.4.8)\n",
      "Collecting httplib2<1.0.0,>=0.19.0 (from google-api-python-client->google-generativeai)\n",
      "  Downloading httplib2-0.31.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client->google-generativeai)\n",
      "  Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client->google-generativeai)\n",
      "  Downloading uritemplate-4.2.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: pyparsing<4,>=3.0.4 in c:\\users\\ommah\\anaconda3\\lib\\site-packages (from httplib2<1.0.0,>=0.19.0->google-api-python-client->google-generativeai) (3.2.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\ommah\\anaconda3\\lib\\site-packages (from pydantic->google-generativeai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in c:\\users\\ommah\\anaconda3\\lib\\site-packages (from pydantic->google-generativeai) (2.27.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\ommah\\anaconda3\\lib\\site-packages (from tqdm->google-generativeai) (0.4.6)\n",
      "Downloading google_generativeai-0.8.5-py3-none-any.whl (155 kB)\n",
      "Downloading google_ai_generativelanguage-0.6.15-py3-none-any.whl (1.3 MB)\n",
      "   ---------------------------------------- 0.0/1.3 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.3/1.3 MB ? eta -:--:--\n",
      "   --------------- ------------------------ 0.5/1.3 MB 2.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.0/1.3 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.3/1.3 MB 1.7 MB/s eta 0:00:00\n",
      "Downloading google_api_core-2.25.1-py3-none-any.whl (160 kB)\n",
      "Downloading grpcio_status-1.71.2-py3-none-any.whl (14 kB)\n",
      "Downloading proto_plus-1.26.1-py3-none-any.whl (50 kB)\n",
      "Downloading google_api_python_client-2.183.0-py3-none-any.whl (14.2 MB)\n",
      "   ---------------------------------------- 0.0/14.2 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/14.2 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.8/14.2 MB 2.2 MB/s eta 0:00:07\n",
      "   --- ------------------------------------ 1.3/14.2 MB 2.3 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 1.8/14.2 MB 2.4 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 2.4/14.2 MB 2.4 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 2.9/14.2 MB 2.5 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 3.4/14.2 MB 2.5 MB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 3.9/14.2 MB 2.5 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 4.7/14.2 MB 2.6 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 5.2/14.2 MB 2.6 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 5.8/14.2 MB 2.6 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 6.0/14.2 MB 2.6 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 6.6/14.2 MB 2.5 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 7.3/14.2 MB 2.6 MB/s eta 0:00:03\n",
      "   ---------------------- ----------------- 7.9/14.2 MB 2.6 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 8.7/14.2 MB 2.7 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 9.2/14.2 MB 2.7 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 9.7/14.2 MB 2.7 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 10.5/14.2 MB 2.7 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 11.0/14.2 MB 2.7 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 11.8/14.2 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 12.3/14.2 MB 2.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 13.1/14.2 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.9/14.2 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.2/14.2 MB 2.8 MB/s eta 0:00:00\n",
      "Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Downloading httplib2-0.31.0-py3-none-any.whl (91 kB)\n",
      "Downloading uritemplate-4.2.0-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: uritemplate, proto-plus, httplib2, grpcio-status, google-auth-httplib2, google-api-core, google-api-python-client, google-ai-generativelanguage, google-generativeai\n",
      "\n",
      "   -------- ------------------------------- 2/9 [httplib2]\n",
      "   ---------------------- ----------------- 5/9 [google-api-core]\n",
      "   ---------------------- ----------------- 5/9 [google-api-core]\n",
      "   -------------------------- ------------- 6/9 [google-api-python-client]\n",
      "   -------------------------- ------------- 6/9 [google-api-python-client]\n",
      "   -------------------------- ------------- 6/9 [google-api-python-client]\n",
      "   -------------------------- ------------- 6/9 [google-api-python-client]\n",
      "   -------------------------- ------------- 6/9 [google-api-python-client]\n",
      "   -------------------------- ------------- 6/9 [google-api-python-client]\n",
      "   ------------------------------- -------- 7/9 [google-ai-generativelanguage]\n",
      "   ------------------------------- -------- 7/9 [google-ai-generativelanguage]\n",
      "   ------------------------------- -------- 7/9 [google-ai-generativelanguage]\n",
      "   ------------------------------- -------- 7/9 [google-ai-generativelanguage]\n",
      "   ------------------------------- -------- 7/9 [google-ai-generativelanguage]\n",
      "   ------------------------------- -------- 7/9 [google-ai-generativelanguage]\n",
      "   ------------------------------- -------- 7/9 [google-ai-generativelanguage]\n",
      "   ------------------------------- -------- 7/9 [google-ai-generativelanguage]\n",
      "   ------------------------------- -------- 7/9 [google-ai-generativelanguage]\n",
      "   ------------------------------- -------- 7/9 [google-ai-generativelanguage]\n",
      "   ------------------------------- -------- 7/9 [google-ai-generativelanguage]\n",
      "   ------------------------------- -------- 7/9 [google-ai-generativelanguage]\n",
      "   ------------------------------- -------- 7/9 [google-ai-generativelanguage]\n",
      "   ----------------------------------- ---- 8/9 [google-generativeai]\n",
      "   ----------------------------------- ---- 8/9 [google-generativeai]\n",
      "   ----------------------------------- ---- 8/9 [google-generativeai]\n",
      "   ---------------------------------------- 9/9 [google-generativeai]\n",
      "\n",
      "Successfully installed google-ai-generativelanguage-0.6.15 google-api-core-2.25.1 google-api-python-client-2.183.0 google-auth-httplib2-0.2.0 google-generativeai-0.8.5 grpcio-status-1.71.2 httplib2-0.31.0 proto-plus-1.26.1 uritemplate-4.2.0\n"
     ]
    }
   ],
   "source": [
    "# import pypdf\n",
    "# print(\"pypdf version:\", pypdf.__version__)\n",
    "\n",
    "# import sys\n",
    "# !{sys.executable} -m pip install tf-keras\n",
    "import sys\n",
    "!{sys.executable} -m pip install google-generativeai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da83315f-cd28-4332-83d9-1873a5166825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 13 pages from 1 PDF files.\n",
      "Split 13 documents into 290 chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ommah\\AppData\\Local\\Temp\\ipykernel_16868\\1407763816.py:54: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ommah\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      " Saved 290 chunks to chroma.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ommah\\AppData\\Local\\Temp\\ipykernel_16868\\1407763816.py:58: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  db.persist()\n"
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings  # <-- NEW\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "# Using local embeddings instead of OpenAI paid api\n",
    "CHROMA_PATH = \"chroma\"\n",
    "DATA_PATH = os.path.join(\"..\", \"..\", \"data\", \"rag_data\")\n",
    "\n",
    "def main():\n",
    "    generate_data_store()\n",
    "\n",
    "def generate_data_store():\n",
    "    documents = load_documents()\n",
    "    chunks = split_text(documents)\n",
    "    save_to_chroma(chunks)\n",
    "\n",
    "def load_documents():\n",
    "    documents = []\n",
    "    pdf_files = glob.glob(os.path.join(DATA_PATH, \"*.pdf\"))  # Load all PDFs\n",
    "    for file_path in pdf_files:\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        documents.extend(loader.load())\n",
    "    print(f\"Loaded {len(documents)} pages from {len(pdf_files)} PDF files.\")\n",
    "    return documents\n",
    "\n",
    "def split_text(documents: list[Document]):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=300,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "        add_start_index=True,\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
    "    return chunks\n",
    "\n",
    "def save_to_chroma(chunks: list[Document]):\n",
    "    # Clear out the database first, but safely\n",
    "    if os.path.exists(CHROMA_PATH):\n",
    "        try:\n",
    "            shutil.rmtree(CHROMA_PATH)\n",
    "        except PermissionError:\n",
    "            print(\"Chroma DB is locked or used by other kernel\")\n",
    "            return\n",
    "\n",
    "    # Use free, local embedding model\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "    # Create a new DB from the documents\n",
    "    db = Chroma.from_documents(chunks, embeddings, persist_directory=CHROMA_PATH)\n",
    "    db.persist()\n",
    "    db = None  # Explicitly drop reference to release file handles\n",
    "    print(f\" Saved {len(chunks)} chunks to {CHROMA_PATH}.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9eb2a1c-d699-4d19-b499-f99d64eec095",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e048bd98-4e12-4017-bbb0-88cdb369d247",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] query_text\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ommah\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import HuggingFaceHub  # free model from HuggingFace Hub\n",
    "\n",
    "CHROMA_PATH = \"chroma\"\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "---\n",
    "\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\"\n",
    "\n",
    "def main():\n",
    "    # CLI for passing question\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"query_text\", type=str, help=\"The query text you want to ask.\")\n",
    "    args = parser.parse_args()\n",
    "    query_text = args.query_text\n",
    "\n",
    "    # Prepare the DB with local embeddings\n",
    "    embedding_function = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)\n",
    "\n",
    "    # Search the DB (top 3 chunks)\n",
    "    results = db.similarity_search_with_relevance_scores(query_text, k=3)\n",
    "    if len(results) == 0 or results[0][1] < 0.3:\n",
    "        print(f\"Unable to find good matching results.\")\n",
    "        return\n",
    "\n",
    "    # Prepare the context for the prompt\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc, _ in results])\n",
    "    prompt = PromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    final_prompt = prompt.format(context=context_text, question=query_text)\n",
    "\n",
    "    # Load a free HuggingFace LLM (e.g. flan-t5-base)\n",
    "    llm = HuggingFaceHub(repo_id=\"google/flan-t5-base\", model_kwargs={\"temperature\": 0, \"max_length\": 512})\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "    response_text = chain.run({\"context\": context_text, \"question\": query_text})\n",
    "\n",
    "    # Print results\n",
    "    sources = [doc.metadata.get(\"source\", None) for doc, _ in results]\n",
    "    formatted_response = f\"Response: {response_text}\\n\\nSources: {sources}\"\n",
    "    print(formatted_response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8385be8-f60c-4b5e-b7d6-670e2854fa0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ sample_context.json created successfully!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "sample_data = {\n",
    "  \"product_name\": \"Copper Wire\",\n",
    "  \"process_route\": \"Mining → Smelting → Casting → Transport\",\n",
    "  \n",
    "  \"user_inputs\": {\n",
    "    \"energy_source\": \"Electricity\",\n",
    "    \"transport_mode\": \"Rail\",\n",
    "    \"transport_distance_km\": 407.64,\n",
    "    \"recycled_content_percent\": 74.52,\n",
    "    \"location\": \"South America\",\n",
    "    \"functional_unit\": \"1 kg Copper Wire\",\n",
    "    \"raw_material_type\": \"Aluminium Scrap\",\n",
    "    \"processing_method\": \"Conventional\"\n",
    "  },\n",
    "  \n",
    "  \"ai_predictions\": {\n",
    "    \"gwp_kg_co2_eq\": 1082.34,\n",
    "    \"material_circularity_indicator\": 0.51,\n",
    "    \"water_consumption_m3\": 12.23,\n",
    "    \"end_of_life_recycling_rate_percent\": 74.77,\n",
    "    \"energy_per_material_mj\": 2.29,\n",
    "    \"total_air_emissions_kg\": 39.36,\n",
    "    \"total_water_emissions_kg\": 1.48,\n",
    "    \"circularity_score\": 50.66,\n",
    "    \"potential_gwp_reduction_renewable_percent\": 15.0,\n",
    "    \"potential_mci_improvement_recycling_percent\": 10.0\n",
    "  },\n",
    "  \n",
    "  \"benchmarks\": {\n",
    "    \"industry_average_gwp\": 1200.0,\n",
    "    \"best_in_class_mci\": 0.8,\n",
    "    \"sector_average_water_m3\": 15.0\n",
    "  }\n",
    "}\n",
    "\n",
    "# Save to a JSON file\n",
    "with open(\"sample_context.json\", \"w\") as f:\n",
    "    json.dump(sample_data, f, indent=2)\n",
    "\n",
    "print(\"✅ sample_context.json created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56efef1-a2e7-4321-9848-4d3c7afc51a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
