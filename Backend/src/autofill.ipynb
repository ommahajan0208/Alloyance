{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f891b79-f2cf-425f-8331-583367bc6317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J\n"
     ]
    }
   ],
   "source": [
    "print('J')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e53a0f9-5c68-4849-a348-087e3ae10771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting new imputer on training subset...\n",
      "Saving fitted imputer and encoders...\n",
      "Imputer ready.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib  # <-- for saving/loading\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# --- Paths ---\n",
    "file_path = os.path.join(\"..\", \"data\", \"lca_dataset.csv\")\n",
    "imputer_path = os.path.join(\"..\", \"model\", \"rf_imputer.pkl\")\n",
    "encoders_path = os.path.join(\"..\", \"model\", \"label_encoders.pkl\")\n",
    "\n",
    "# Ensure models directory exists\n",
    "os.makedirs(os.path.dirname(imputer_path), exist_ok=True)\n",
    "\n",
    "# --- Load dataset ---\n",
    "if not os.path.exists(file_path):\n",
    "    raise FileNotFoundError(f\"Dataset not found at {file_path}\")\n",
    "\n",
    "df_training = pd.read_csv(file_path)\n",
    "\n",
    "# 1. Separate categorical and numeric columns\n",
    "categorical_cols = df_training.select_dtypes(include=['object']).columns.tolist()\n",
    "numeric_cols = df_training.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# 2. Encode categorical columns\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df_training[col] = le.fit_transform(df_training[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# --- Load or Fit Imputer ---\n",
    "if os.path.exists(imputer_path) and os.path.exists(encoders_path):\n",
    "    print(\"Loading saved imputer and encoders...\")\n",
    "    imputer = joblib.load(imputer_path)\n",
    "    label_encoders = joblib.load(encoders_path)\n",
    "else:\n",
    "    print(\"Fitting new imputer on training subset...\")\n",
    "    imputer = IterativeImputer( estimator=RandomForestRegressor(n_estimators=50, random_state=42)\n",
    "                               , max_iter=10, \n",
    "                               random_state=0 \n",
    "    )\n",
    "    imputer.fit(df_training)\n",
    "    # imputer.fit(df_training.sample(500, random_state=42))  # subset for faster training\n",
    "    print(\"Saving fitted imputer and encoders...\")\n",
    "    joblib.dump(imputer, imputer_path)\n",
    "    joblib.dump(label_encoders, encoders_path)\n",
    "\n",
    "print(\"Imputer ready.\")\n",
    "\n",
    "def autofill_lca_data(json_input):\n",
    "    user_data = json.loads(json_input)\n",
    "    df_user = pd.DataFrame([user_data])\n",
    "\n",
    "    # --- Simple rule-based defaults ---\n",
    "    if \"Process Stage\" in df_user.columns and df_user[\"Process Stage\"].iloc[0] == \"End-of-Life\":\n",
    "        if \"End-of-Life Treatment\" not in df_user or pd.isna(df_user[\"End-of-Life Treatment\"]).any():\n",
    "            df_user[\"End-of-Life Treatment\"] = \"Recycling\"\n",
    "\n",
    "    # Align columns\n",
    "    df_user = df_user.reindex(columns=df_training.columns, fill_value=np.nan)\n",
    "\n",
    "    # Encode categorical columns\n",
    "    for col in categorical_cols:\n",
    "        if col in df_user.columns:\n",
    "            df_user[col] = df_user[col].apply(\n",
    "                lambda x: label_encoders[col].transform([x])[0]\n",
    "                if pd.notna(x) and x in label_encoders[col].classes_\n",
    "                else np.nan\n",
    "            )\n",
    "\n",
    "    print(\"Starting imputation for user data...\")\n",
    "    imputed_array = imputer.transform(df_user)\n",
    "    print(\"Imputation complete.\")\n",
    "\n",
    "    df_imputed = pd.DataFrame(imputed_array, columns=df_user.columns)\n",
    "\n",
    "    # Decode categorical columns\n",
    "    for col in categorical_cols:\n",
    "        df_imputed[col] = df_imputed[col].round().astype(int)\n",
    "        valid_classes = label_encoders[col].classes_\n",
    "        df_imputed[col] = df_imputed[col].map(\n",
    "            lambda x: valid_classes[x] if x < len(valid_classes) else \"Unknown\"\n",
    "        )\n",
    "\n",
    "    return df_imputed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dbc08e6-0b18-4dbb-a8f8-44d84f96c081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting imputation for user data...\n",
      "Imputation complete.\n",
      "Final Autofilled DataFrame:\n",
      "  Process Stage Technology Time Period Location       Functional Unit  \\\n",
      "0           Use   Advanced   2015-2019   Europe  1 m2 Aluminium Panel   \n",
      "\n",
      "  Raw Material Type  Raw Material Quantity (kg or unit) Energy Input Type  \\\n",
      "0   Aluminium Scrap                          999.995117       Natural Gas   \n",
      "\n",
      "   Energy Input Quantity (MJ) Processing Method  ... GHG_per_Material  \\\n",
      "0                13757.681641          Advanced  ...         4.929952   \n",
      "\n",
      "   Time_Period_Numeric   Total_Cost Circularity_Score Circular_Economy_Index  \\\n",
      "0           2016.92749  2165.173828            40.173               0.400002   \n",
      "\n",
      "   Recycled Content (%)  Resource Efficiency (%)  \\\n",
      "0             10.052256                 11.21825   \n",
      "\n",
      "   Extended Product Life (years)  Recovery Rate (%)  Reuse Potential (%)  \n",
      "0                      27.848068          11.937099             0.611402  \n",
      "\n",
      "[1 rows x 45 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage\n",
    "user_json = '''\n",
    "{\n",
    "  \"Process Stage\": \"Use\",\n",
    "  \"Technology\": \"Advanced\",\n",
    "  \"Location\": \"Europe\",\n",
    "  \"Raw Material Quantity (kg or unit)\": null,\n",
    "  \"Energy Input Quantity (MJ)\": null,\n",
    "  \"Transport Distance (km)\": 500,\n",
    "  \"Emissions to Air CO2 (kg)\": null\n",
    "}\n",
    "'''\n",
    "\n",
    "final_df = autofill_lca_data(user_json)\n",
    "print(\"Final Autofilled DataFrame:\")\n",
    "print(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae58ee35-0394-46a0-af34-4048526c693e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# --- Load dataset ---\n",
    "file_path = os.path.join(\"..\", \"data\", \"lca_dataset.csv\")\n",
    "if not os.path.exists(file_path):\n",
    "    raise FileNotFoundError(f\"Dataset not found at {file_path}\")\n",
    "\n",
    "df_training = pd.read_csv(file_path)\n",
    "\n",
    "# --- Separate categorical and numeric columns ---\n",
    "categorical_cols = df_training.select_dtypes(include=['object']).columns.tolist()\n",
    "numeric_cols = df_training.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# --- Encode categorical columns ---\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    df_training[col] = le.fit_transform(df_training[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# --- Function to evaluate imputer ---\n",
    "def evaluate_imputer(df, numeric_cols, sample_frac=0.2, mask_fraction=0.2):\n",
    "    \"\"\"\n",
    "    Evaluates IterativeImputer performance by masking known values and imputing them.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for col in numeric_cols:\n",
    "        # Work on a sample for speed\n",
    "        df_eval = df.sample(n=2000, random_state=42) \n",
    "        # df_eval = df.sample(frac=sample_frac, random_state=42).copy()\n",
    "        mask = df_eval[col].notna()\n",
    "        if mask.sum() < 5:\n",
    "            continue  # skip columns with too few non-null values\n",
    "\n",
    "        mask_idx = np.random.choice(df_eval[mask].index, \n",
    "                                    size=int(mask_fraction * mask.sum()), \n",
    "                                    replace=False)\n",
    "        true_values = df_eval.loc[mask_idx, col]\n",
    "        df_eval.loc[mask_idx, col] = np.nan\n",
    "\n",
    "        # Fit imputer on this masked dataset\n",
    "        imputer = IterativeImputer(\n",
    "            estimator=XGBRegressor(\n",
    "                n_estimators=100,\n",
    "                max_depth=5,\n",
    "                learning_rate=0.1,\n",
    "                n_jobs=-1,\n",
    "                tree_method=\"hist\",\n",
    "                random_state=42\n",
    "            ),\n",
    "            max_iter=10,\n",
    "            random_state=0\n",
    "        )\n",
    "        imputed_array = imputer.fit_transform(df_eval)\n",
    "        df_imputed = pd.DataFrame(imputed_array, columns=df_eval.columns)\n",
    "        predicted_values = df_imputed.loc[mask_idx, col]\n",
    "\n",
    "        rmse = np.sqrt(mean_squared_error(true_values, predicted_values))\n",
    "        r2 = r2_score(true_values, predicted_values)\n",
    "        results.append({\"Column\": col, \"RMSE\": rmse, \"R2\": r2})\n",
    "\n",
    "    results_df = pd.DataFrame(results).sort_values(\"R2\", ascending=False)\n",
    "    return results_df\n",
    "\n",
    "# --- Run evaluation ---\n",
    "metrics_df = evaluate_imputer(df_training, numeric_cols, sample_frac=0.5)\n",
    "print(\"=== Imputer Evaluation Metrics ===\")\n",
    "print(metrics_df)\n",
    "\n",
    "# --- Plot true vs. imputed for best column ---\n",
    "if not metrics_df.empty:\n",
    "    best_col = metrics_df.iloc[0][\"Column\"]\n",
    "    print(f\"\\nPlotting True vs Imputed for best-performing column: {best_col}\")\n",
    "\n",
    "    # Mask and re-impute just for plotting\n",
    "    df_eval = df_training.sample(frac=0.5, random_state=42).copy()\n",
    "    mask = df_eval[best_col].notna()\n",
    "    mask_idx = np.random.choice(df_eval[mask].index, size=int(0.2 * mask.sum()), replace=False)\n",
    "    true_values = df_eval.loc[mask_idx, best_col]\n",
    "    df_eval.loc[mask_idx, best_col] = np.nan\n",
    "\n",
    "    imputer = IterativeImputer(\n",
    "        estimator=RandomForestRegressor(n_estimators=50, random_state=42),\n",
    "        max_iter=10,\n",
    "        random_state=0\n",
    "    )\n",
    "    imputed_array = imputer.fit_transform(df_eval)\n",
    "    df_imputed = pd.DataFrame(imputed_array, columns=df_eval.columns)\n",
    "    predicted_values = df_imputed.loc[mask_idx, best_col]\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(true_values, predicted_values, alpha=0.5)\n",
    "    plt.xlabel(\"True Values\")\n",
    "    plt.ylabel(\"Imputed Values\")\n",
    "    plt.title(f\"True vs. Imputed: {best_col}\")\n",
    "    plt.plot([true_values.min(), true_values.max()],\n",
    "             [true_values.min(), true_values.max()],\n",
    "             color='red', linestyle='--')\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No numeric columns available for evaluation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87f0b40-d941-48a0-bd6c-62cfcaf2a6f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fba6d5c-a59b-45b6-a698-9a971ecebb02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e78e295-0aa8-4f15-aba4-a35a450da477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For maximum speed (20-30 seconds):\n",
    "# chunk_size = 10000  # Larger chunks\n",
    "# max_cols_to_eval = 3  # Evaluate fewer columns\n",
    "\n",
    "# # For better quality (2-3 minutes):\n",
    "# chunk_size = 3000  # Smaller chunks\n",
    "# max_iter = 5  # More iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616329d0-fa74-4cdf-b380-3325cd6254f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26626142-4651-48c6-95d9-5e3141147722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer, SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Configuration for different dataset sizes ---\n",
    "class ImputationConfig:\n",
    "    \"\"\"Configuration based on dataset size\"\"\"\n",
    "    def __init__(self, n_rows):\n",
    "        self.n_rows = n_rows\n",
    "        \n",
    "        if n_rows < 5000:\n",
    "            self.eval_sample_size = min(500, n_rows)\n",
    "            self.max_iter = 5\n",
    "            self.n_jobs = 4\n",
    "            self.estimator = BayesianRidge()\n",
    "        elif n_rows < 15000:\n",
    "            self.eval_sample_size = 300\n",
    "            self.max_iter = 3\n",
    "            self.n_jobs = 6\n",
    "            self.estimator = BayesianRidge()\n",
    "        else:  # For 25000+ rows\n",
    "            self.eval_sample_size = 200\n",
    "            self.max_iter = 2\n",
    "            self.n_jobs = 8\n",
    "            # Use simpler estimator for very large datasets\n",
    "            self.estimator = BayesianRidge(alpha_1=1e-06, lambda_1=1e-06)\n",
    "\n",
    "# --- Load dataset ---\n",
    "file_path = os.path.join(\"..\", \"data\", \"lca_dataset.csv\")\n",
    "if not os.path.exists(file_path):\n",
    "    raise FileNotFoundError(f\"Dataset not found at {file_path}\")\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "df_training = pd.read_csv(file_path)\n",
    "print(f\"Dataset shape: {df_training.shape}\")\n",
    "\n",
    "# Initialize configuration based on dataset size\n",
    "config = ImputationConfig(len(df_training))\n",
    "print(f\"Configuration: sample_size={config.eval_sample_size}, max_iter={config.max_iter}, n_jobs={config.n_jobs}\")\n",
    "\n",
    "# --- Separate categorical and numeric columns ---\n",
    "categorical_cols = df_training.select_dtypes(include=['object']).columns.tolist()\n",
    "numeric_cols = df_training.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "print(f\"Numeric columns: {len(numeric_cols)}, Categorical columns: {len(categorical_cols)}\")\n",
    "\n",
    "# --- Encode categorical columns efficiently ---\n",
    "print(\"Encoding categorical columns...\")\n",
    "label_encoders = {}\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    # Use category dtype for memory efficiency with large datasets\n",
    "    df_training[col] = le.fit_transform(df_training[col].astype(str)).astype('category')\n",
    "    label_encoders[col] = le\n",
    "\n",
    "# --- Identify columns with missing values ---\n",
    "missing_cols = [col for col in numeric_cols if df_training[col].isnull().any()]\n",
    "complete_cols = [col for col in numeric_cols if not df_training[col].isnull().any()]\n",
    "print(f\"Columns with missing values: {len(missing_cols)}\")\n",
    "print(f\"Complete columns: {len(complete_cols)}\")\n",
    "\n",
    "# --- Fast correlation-based feature selection ---\n",
    "def select_correlated_features(df, target_col, n_features=10):\n",
    "    \"\"\"Select most correlated features for imputation\"\"\"\n",
    "    # Use complete columns for correlation calculation\n",
    "    available_cols = [col for col in df.columns if col != target_col]\n",
    "    \n",
    "    # Calculate correlations only with columns that have enough data\n",
    "    correlations = {}\n",
    "    for col in available_cols:\n",
    "        if df[col].notna().sum() > len(df) * 0.5:  # At least 50% non-null\n",
    "            try:\n",
    "                corr = df[target_col].corr(df[col])\n",
    "                if not np.isnan(corr):\n",
    "                    correlations[col] = abs(corr)\n",
    "            except:\n",
    "                pass\n",
    "    \n",
    "    # Select top correlated features\n",
    "    sorted_corrs = sorted(correlations.items(), key=lambda x: x[1], reverse=True)\n",
    "    selected_features = [col for col, _ in sorted_corrs[:n_features]]\n",
    "    return selected_features\n",
    "\n",
    "# --- Hybrid imputation strategy ---\n",
    "def hybrid_impute(df, numeric_cols, config):\n",
    "    \"\"\"\n",
    "    Hybrid approach: SimpleImputer for initial fill, then IterativeImputer for refinement\n",
    "    \"\"\"\n",
    "    print(\"\\nStarting hybrid imputation...\")\n",
    "    df_imputed = df.copy()\n",
    "    \n",
    "    # Step 1: Quick initial imputation with SimpleImputer\n",
    "    print(\"Step 1: Initial simple imputation...\")\n",
    "    simple_imputer = SimpleImputer(strategy='median')\n",
    "    df_imputed[numeric_cols] = simple_imputer.fit_transform(df_imputed[numeric_cols])\n",
    "    \n",
    "    # Step 2: Identify columns that need refinement (high missing %)\n",
    "    missing_percentages = df[numeric_cols].isnull().sum() / len(df)\n",
    "    cols_to_refine = missing_percentages[missing_percentages > 0.1].index.tolist()\n",
    "    \n",
    "    if cols_to_refine:\n",
    "        print(f\"Step 2: Refining {len(cols_to_refine)} columns with >10% missing values...\")\n",
    "        \n",
    "        # Use IterativeImputer only on columns that need it\n",
    "        for col in cols_to_refine[:5]:  # Limit to top 5 most missing columns for speed\n",
    "            print(f\"  Refining {col} ({missing_percentages[col]:.1%} missing)...\")\n",
    "            \n",
    "            # Select correlated features for this column\n",
    "            selected_features = select_correlated_features(df, col, n_features=10)\n",
    "            if col in selected_features:\n",
    "                selected_features.remove(col)\n",
    "            selected_features.append(col)\n",
    "            \n",
    "            # Impute using only selected features\n",
    "            subset_df = df_imputed[selected_features].copy()\n",
    "            \n",
    "            iterative_imputer = IterativeImputer(\n",
    "                estimator=config.estimator,\n",
    "                max_iter=config.max_iter,\n",
    "                initial_strategy='median',\n",
    "                random_state=42\n",
    "            )\n",
    "            \n",
    "            imputed_subset = iterative_imputer.fit_transform(subset_df)\n",
    "            df_imputed[col] = imputed_subset[:, selected_features.index(col)]\n",
    "    \n",
    "    return df_imputed\n",
    "\n",
    "# --- Optimized evaluation for large datasets ---\n",
    "def evaluate_imputer_large_dataset(df, missing_cols, config, max_cols_to_eval=10):\n",
    "    \"\"\"\n",
    "    Evaluation optimized for large datasets\n",
    "    \"\"\"\n",
    "    print(f\"\\nEvaluating imputation quality on sample of {config.eval_sample_size} rows...\")\n",
    "    \n",
    "    # Sample once for all evaluations\n",
    "    df_sample = df.sample(n=min(config.eval_sample_size, len(df)), random_state=42)\n",
    "    \n",
    "    # Limit evaluation to most important columns\n",
    "    cols_to_evaluate = missing_cols[:max_cols_to_eval]\n",
    "    \n",
    "    results = []\n",
    "    for i, col in enumerate(cols_to_evaluate, 1):\n",
    "        print(f\"Evaluating {i}/{len(cols_to_evaluate)}: {col}\")\n",
    "        \n",
    "        mask = df_sample[col].notna()\n",
    "        if mask.sum() < 20:\n",
    "            continue\n",
    "        \n",
    "        # Create test set\n",
    "        df_test = df_sample.copy()\n",
    "        test_size = min(50, int(0.2 * mask.sum()))\n",
    "        mask_idx = np.random.choice(df_test[mask].index, size=test_size, replace=False)\n",
    "        true_values = df_test.loc[mask_idx, col].values\n",
    "        df_test.loc[mask_idx, col] = np.nan\n",
    "        \n",
    "        # Quick imputation for evaluation\n",
    "        simple_imputer = SimpleImputer(strategy='median')\n",
    "        imputed_values = simple_imputer.fit_transform(df_test[[col]])\n",
    "        predicted_values = imputed_values[mask_idx, 0]\n",
    "        \n",
    "        rmse = np.sqrt(mean_squared_error(true_values, predicted_values))\n",
    "        r2 = r2_score(true_values, predicted_values) if len(true_values) > 1 else 0\n",
    "        \n",
    "        results.append({\n",
    "            \"Column\": col,\n",
    "            \"Missing %\": f\"{df[col].isnull().sum() / len(df) * 100:.1f}%\",\n",
    "            \"RMSE\": rmse,\n",
    "            \"R2\": r2\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results).sort_values(\"R2\", ascending=False)\n",
    "\n",
    "# --- Chunked imputation for very large datasets ---\n",
    "def chunked_impute(df, numeric_cols, chunk_size=5000):\n",
    "    \"\"\"\n",
    "    Process dataset in chunks for memory efficiency\n",
    "    \"\"\"\n",
    "    print(f\"\\nProcessing dataset in chunks of {chunk_size} rows...\")\n",
    "    n_chunks = (len(df) - 1) // chunk_size + 1\n",
    "    \n",
    "    df_imputed_list = []\n",
    "    for i in range(n_chunks):\n",
    "        start_idx = i * chunk_size\n",
    "        end_idx = min((i + 1) * chunk_size, len(df))\n",
    "        print(f\"Processing chunk {i+1}/{n_chunks} (rows {start_idx}-{end_idx})...\")\n",
    "        \n",
    "        chunk = df.iloc[start_idx:end_idx].copy()\n",
    "        \n",
    "        # Simple imputation for each chunk\n",
    "        simple_imputer = SimpleImputer(strategy='median')\n",
    "        chunk[numeric_cols] = simple_imputer.fit_transform(chunk[numeric_cols])\n",
    "        df_imputed_list.append(chunk)\n",
    "    \n",
    "    return pd.concat(df_imputed_list, ignore_index=True)\n",
    "\n",
    "# --- Main execution ---\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"OPTIMIZED IMPUTATION FOR LARGE DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Choose strategy based on dataset size\n",
    "if len(df_training) > 20000:\n",
    "    print(\"\\nðŸš€ Using FAST MODE for large dataset (25,000+ rows)\")\n",
    "    print(\"Strategy: Chunked processing + Simple imputation\")\n",
    "    \n",
    "    # Option 1: Chunked simple imputation (fastest)\n",
    "    df_imputed = chunked_impute(df_training, numeric_cols, chunk_size=5000)\n",
    "    \n",
    "elif len(df_training) > 10000:\n",
    "    print(\"\\nâš¡ Using BALANCED MODE for medium-large dataset\")\n",
    "    print(\"Strategy: Hybrid imputation\")\n",
    "    \n",
    "    # Option 2: Hybrid imputation\n",
    "    df_imputed = hybrid_impute(df_training, numeric_cols, config)\n",
    "    \n",
    "else:\n",
    "    print(\"\\nðŸŽ¯ Using QUALITY MODE for smaller dataset\")\n",
    "    print(\"Strategy: Full iterative imputation\")\n",
    "    \n",
    "    # Option 3: Full iterative imputation (best quality, slower)\n",
    "    imputer = IterativeImputer(\n",
    "        estimator=config.estimator,\n",
    "        max_iter=config.max_iter,\n",
    "        random_state=42\n",
    "    )\n",
    "    df_imputed = df_training.copy()\n",
    "    df_imputed[numeric_cols] = imputer.fit_transform(df_imputed[numeric_cols])\n",
    "\n",
    "imputation_time = time.time() - start_time\n",
    "print(f\"\\nâœ… Imputation completed in {imputation_time:.2f} seconds\")\n",
    "\n",
    "# --- Quick evaluation ---\n",
    "if missing_cols:\n",
    "    print(\"\\nRunning quick evaluation...\")\n",
    "    metrics_df = evaluate_imputer_large_dataset(df_training, missing_cols, config, max_cols_to_eval=5)\n",
    "    print(\"\\n=== Top 5 Imputation Results ===\")\n",
    "    print(metrics_df)\n",
    "\n",
    "# --- Memory-efficient visualization ---\n",
    "print(\"\\nGenerating visualization...\")\n",
    "if len(missing_cols) >= 3:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    for idx, ax in enumerate(axes[:3]):\n",
    "        col = missing_cols[idx]\n",
    "        \n",
    "        # Use very small sample for visualization\n",
    "        viz_sample_size = 100\n",
    "        df_viz = df_training.sample(n=viz_sample_size, random_state=42)\n",
    "        mask = df_viz[col].notna()\n",
    "        \n",
    "        if mask.sum() > 10:\n",
    "            # Compare simple imputation results\n",
    "            df_test = df_viz.copy()\n",
    "            test_idx = mask[mask].sample(n=min(20, mask.sum()//2)).index\n",
    "            true_values = df_test.loc[test_idx, col]\n",
    "            df_test.loc[test_idx, col] = np.nan\n",
    "            \n",
    "            simple_imputer = SimpleImputer(strategy='median')\n",
    "            imputed = simple_imputer.fit_transform(df_test[[col]])\n",
    "            predicted_values = pd.Series(imputed[:, 0], index=df_test.index).loc[test_idx]\n",
    "            \n",
    "            ax.scatter(true_values, predicted_values, alpha=0.6, s=30)\n",
    "            ax.plot([true_values.min(), true_values.max()],\n",
    "                    [true_values.min(), true_values.max()],\n",
    "                    'r--', alpha=0.5)\n",
    "            ax.set_xlabel(\"True Values\", fontsize=9)\n",
    "            ax.set_ylabel(\"Imputed Values\", fontsize=9)\n",
    "            ax.set_title(f\"{col[:20]}...\\nMissing: {df_training[col].isnull().sum()}\", fontsize=10)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --- Save imputed dataset ---\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Missing value comparison\n",
    "print(\"\\nMissing values BEFORE imputation:\")\n",
    "before_missing = df_training[numeric_cols].isnull().sum()\n",
    "print(before_missing[before_missing > 0].head(10))\n",
    "\n",
    "print(\"\\nMissing values AFTER imputation:\")\n",
    "after_missing = df_imputed[numeric_cols].isnull().sum()\n",
    "print(f\"Total: {after_missing.sum()} (should be 0)\")\n",
    "\n",
    "# Save option\n",
    "save_response = input(\"\\nðŸ’¾ Save imputed dataset? (y/n): \")\n",
    "if save_response.lower() == 'y':\n",
    "    output_path = os.path.join(\"..\", \"data\", \"lca_dataset_imputed_fast.csv\")\n",
    "    \n",
    "    # Decode categorical columns\n",
    "    df_save = df_imputed.copy()\n",
    "    for col in categorical_cols:\n",
    "        if col in label_encoders:\n",
    "            df_save[col] = label_encoders[col].inverse_transform(\n",
    "                df_imputed[col].astype(int)\n",
    "            )\n",
    "    \n",
    "    df_save.to_csv(output_path, index=False)\n",
    "    print(f\"âœ… Saved to: {output_path}\")\n",
    "    print(f\"   File size: {os.path.getsize(output_path) / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Performance summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Dataset size: {len(df_training):,} rows Ã— {len(df_training.columns)} columns\")\n",
    "print(f\"Imputation time: {imputation_time:.2f} seconds\")\n",
    "print(f\"Speed: {len(df_training) / imputation_time:.0f} rows/second\")\n",
    "print(f\"Estimated time for 25,000 rows: {25000 / (len(df_training) / imputation_time):.1f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94c1d87-ec68-47e4-9851-9a6b22195fba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
